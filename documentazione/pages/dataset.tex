\section{Dataset}

The dataset used in this project is \textbf{MovieLens 1M}, a collection of explicit movie ratings widely used in recommender systems research.

\subsection{Dataset Characteristics}

MovieLens 1M contains:
\begin{itemize}
    \item \textbf{1,000,209 explicit ratings}
    \item \textbf{6,040 users}
    \item \textbf{3,952 movies} 
\end{itemize}


\subsection{Multimodal Extension}

In the context of this project, the MovieLens 1M dataset has been enriched with multimodal data for each movie:

\begin{itemize}
    \item \textbf{Images}: movie posters or representative images (.jpg format)
    \item \textbf{Audio}: audio clips or associated soundtracks (.wav format)
    \item \textbf{Texts}: textual descriptions, synopses, or reviews (.txt format)
\end{itemize}

All multimodal files are organized with the same basename corresponding to the Movie ID, ensuring correspondence between different modalities. For example, for the movie with ID \texttt{1}:
\begin{itemize}
    \item Image: \texttt{ml1m/\_images/1.jpg}
    \item Audio: \texttt{ml1m/\_audios/1.wav}
    \item Text: \texttt{ml1m/\_texts/1.txt}
\end{itemize}

\subsection{Multimodal Embeddings}

The multimodal embeddings were extracted using the \textbf{AudioCLIP} (see Section \ref{sec:AudioClip}) 
\subsubsection{Extraction Process}

The embedding extraction process is implemented in a script, which ensures robust handling of multimodal data through the following pipeline:

\paragraph{File Discovery and Alignment}
The script first scans three directories with raw data and identifies files by their basename (e.g., movie ID). From the initial scan:
\begin{itemize}
    \item 3,196 image files were found
    \item 3,535 audio files were found
    \item 3,197 text files were found
    \item 3,635 unique names across all modalities
\end{itemize}

The script performs an \textbf{intersection} of basenames to ensure only movies with \textit{all three modalities} are processed, resulting in \textbf{3,096 candidate items}.

\paragraph{Per-Modality Extraction with Error Handling}
The extraction pipeline processes each modality with specific procedures:

\begin{itemize}
    \item \textbf{Images}: Resized to 224Ã—224, normalized with CLIP statistics, batch processed
    \item \textbf{Audio}: Sliding window approach (2s windows, 1s stride), mean aggregation across windows
    \item \textbf{Text}: Adaptive chunking for 77-token CLIP limit, sentence-based splitting,
    \item \textbf{Error handling}: Corrupted files logged and excluded from final dataset
\end{itemize}

\paragraph{Consistency Enforcement}
After extraction, the script enforces \textbf{strict alignment}:
\begin{itemize}
    \item Only items successfully extracted in \textit{all three modalities} are retained
    \item Failed extractions (e.g., 1 audio file corrupted) result in removal from all modalities
    \item Final output arrays have \textbf{identical row counts} and order
\end{itemize}

In the current extraction:
\begin{itemize}
    \item 3,096 items had all three modalities available
    \item 1 audio file failed extraction (corrupted EOF)
    \item Final dataset: \textbf{3,095 items} with complete embeddings
\end{itemize}

\subsubsection{Output Files}

The extracted embeddings are saved in NumPy format (.npy)

\begin{itemize}
    \item \texttt{images.npy}: visual embeddings, shape $(3095, 1024)$
    \item \texttt{audios.npy}: audio embeddings, shape $(3095, 1024)$
    \item \texttt{texts.npy}: textual embeddings, shape $(3095, 1024)$
    \item \texttt{item\_features.csv}: mapping between Movie ID and array index
    \item \textbf{row $i$} in each file corresponds to the same movie
\end{itemize}


\subsection{Dataset Filtering and Remapping}

After extracting multimodal embeddings, the interaction dataset (\texttt{movielens\_1m.inter}) undergoes a comprehensive filtering and remapping pipeline. This is necessary in order to properly use the MMRec framework

\subsubsection{Pipeline Overview}
The script first loads the original interaction file. Then, to ensure consistency with the extracted embeddings, the script filters interactions to include only movies present in \texttt{item\_features.csv} (i.e., movies with all three modalities: audio, images, and text).

After this filtering:
\begin{itemize}
    \item Valid items with complete multimodal features: \textbf{3,095}
    \item Remaining interactions: depends on how many ratings reference the 3,095 valid movies
\end{itemize}

\paragraph{Step 3: Core-k Filtering}
To address data sparsity and ensure sufficient interaction density, the script applies \textbf{5-core filtering}, an iterative process that removes users and items with fewer than $k=5$ interactions until convergence.

\textbf{Algorithm:}
\begin{enumerate}
    \item Count interactions per user and per item
    \item Remove users with $< 5$ interactions
    \item Remove items with $< 5$ interactions
    \item Repeat steps 1--3 until no further removals occur (convergence)
\end{enumerate}


\paragraph{Step 4: ID Remapping}
After filtering, user and item IDs may no longer be contiguous (e.g., user IDs might be [1, 5, 12, ...]). To ensure compatibility with matrix-based recommender systems, the script remaps all IDs to contiguous ranges starting from 0.

\textbf{User remapping:}
\begin{itemize}
    \item Original IDs: arbitrary integers
    \item New IDs: $0, 1, 2, \ldots, m-1$ (where $m$ is the number of users after filtering)
\end{itemize}

\textbf{Item remapping:}
\begin{itemize}
    \item Original IDs: arbitrary integers
    \item New IDs: $0, 1, 2, \ldots, n-1$ (where $n$ is the number of items after filtering)
\end{itemize}

\paragraph{Step 5: Embedding Reindexing}
The original \texttt{.npy} files (\texttt{audios.npy}, \texttt{images.npy}, \texttt{texts.npy}) are indexed according to the pre-filtering item set. After remapping, the script reconstructs the embedding arrays to match the new item indices:

\begin{enumerate}
    \item For each new item index $i \in [0, n-1]$:
    \begin{itemize}
        \item Lookup the original item ID from the inverse mapping
        \item Find the old index in \texttt{item\_features.csv}
        \item Copy the corresponding embedding vectors from the original \texttt{.npy} files
    \end{itemize}
    \item Save the reindexed embeddings as:
    \begin{itemize}
        \item \texttt{audio\_filtered.npy}
        \item \texttt{image\_filtered.npy}
        \item \texttt{text\_filtered.npy}
    \end{itemize}
\end{enumerate}

\paragraph{Step 6: Output Files}
The script generates the following files:

\begin{itemize}
    \item \texttt{movielens\_1m\_filtered.inter}: filtered interaction file with remapped IDs
    \item \texttt{audio\_filtered.npy}: reindexed audio embeddings, shape $(n, 1024)$
    \item \texttt{image\_filtered.npy}: reindexed image embeddings, shape $(n, 1024)$
    \item \texttt{text\_filtered.npy}: reindexed text embeddings, shape $(n, 1024)$
    \item \texttt{user\_mapping.csv}: mapping between old and new user IDs
    \item \texttt{item\_mapping.csv}: mapping between old and new item IDs
    \item \texttt{item\_features\_filtered.csv}: updated item-to-index mapping
\end{itemize}


