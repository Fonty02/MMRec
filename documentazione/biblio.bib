@Inbook{Lops2011,
author="Lops, Pasquale
and de Gemmis, Marco
and Semeraro, Giovanni",
editor="Ricci, Francesco
and Rokach, Lior
and Shapira, Bracha
and Kantor, Paul B.",
title="Content-based Recommender Systems: State of the Art and Trends",
bookTitle="Recommender Systems Handbook",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="73--105",
abstract="Recommender systems have the effect of guiding users in a personalized way to interesting objects in a large space of possible options. Content-based recommendation systems try to recommend items similar to those a given user has liked in the past. Indeed, the basic process performed by a content-based recommender consists in matching up the attributes of a user profile in which preferences and interests are stored, with the attributes of a content object (item), in order to recommend to the user new interesting items. This chapter provides an overview of content-based recommender systems, with the aim of imposing a degree of order on the diversity of the different aspects involved in their design and implementation. The first part of the chapter presents the basic concepts and terminology of contentbased recommender systems, a high level architecture, and their main advantages and drawbacks. The second part of the chapter provides a review of the state of the art of systems adopted in several application domains, by thoroughly describing both classical and advanced techniques for representing items and user profiles. The most widely adopted techniques for learning user profiles are also presented. The last part of the chapter discusses trends and future research which might lead towards the next generation of systems, by describing the role of User Generated Content as a way for taking into account evolving vocabularies, and the challenge of feeding users with serendipitous recommendations, that is to say surprisingly interesting items that they might not have otherwise discovered.",
isbn="978-0-387-85820-3",
doi="10.1007/978-0-387-85820-3_3",
url="https://doi.org/10.1007/978-0-387-85820-3_3"
}

@inproceedings{Spillong,
author = {Spillo, Giuseppe and Musacchio, Elio and Musto, Cataldo and de Gemmis, Marco and Lops, Pasquale and Semeraro, Giovanni},
title = {See the Movie, Hear the Song, Read the Book: Extending MovieLens-1M, Last.fm-2K, and DBbook with Multimodal Data},
year = {2025},
isbn = {9798400713644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705328.3748162},
doi = {10.1145/3705328.3748162},
abstract = {The last few years have seen an increasing interest of the RecSys community in the multimodal recommendation research field, as shown by the numerous contributions proposed in the literature. Our paper falls in this research line, as we released a multimodal extension of three state-of-the-art datasets (MovieLens-1M, DBbook, Last.fm-2K) in the movie, book, music recommendation domains, respectively. Although these datasets have been widely adopted for classical recommendation tasks (e.g., collaborative filtering), their use in multimodal recommendation has been hindered by the absence of multimodal information. To fill this gap, we have manually collected multimodal item raw files from different modalities (text, images, audio, and video, when available) for each dataset.Specifically, we have collected, for MovieLens-1M, movie plots (textual information), movie posters (images) and movie trailers (audio and video); for Last.fm-2K, we have collected, for each artist, the tags provided by users (textual information), the most popular album covers (images), and the most popular songs (audio); finally, for DBbook we have collected book abstracts (textual information) and book covers (image). We encoded all this information using state-of-the-art feature encoders, and we released the extended datasets, which include the mappings to the raw multimodal information and the encoded features. Finally, we conduct a benchmark analysis of various recommendation models using MMRec as a multimodal recommendation framework. Our results show that multimodal information can further enhance the quality of recommendations in these domains compared to single collaborative filtering. We release the multimodal version of such datasets to foster this research line, including links to download the raw multimodal files and the encoded item features.},
booktitle = {Proceedings of the Nineteenth ACM Conference on Recommender Systems},
pages = {847–856},
numpages = {10},
keywords = {Recommender Systems, multimodal Recommender Systems},
location = {
},
series = {RecSys '25}
}


@inproceedings{ColdStart,
  title={A Monte Carlo algorithm for cold start recommendation},
  author={Rong, Yu and Wen, Xiao and Cheng, Hong},
  booktitle={Proceedings of the 23rd international conference on World wide web},
  pages={327--336},
  year={2014}
}


@inproceedings{Sparsity,
  title={Research problems in recommender systems},
  author={Mishra, Nitin and Chaturvedi, Saumya and Vij, Aanchal and Tripathi, Sunita},
  booktitle={Journal of Physics: Conference Series},
  volume={1717},
  number={1},
  pages={012002},
  year={2021},
  organization={IOP publishing}
}



@Article{ConcatFeature,
AUTHOR = {Chen, Xi and Lu, Yangsiyi and Wang, Yuehai and Yang, Jianyi},
TITLE = {CMBF: Cross-Modal-Based Fusion Recommendation Algorithm},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5275},
URL = {https://www.mdpi.com/1424-8220/21/16/5275},
PubMedID = {34450716},
ISSN = {1424-8220},
ABSTRACT = {A recommendation system is often used to recommend items that may be of interest to users. One of the main challenges is that the scarcity of actual interaction data between users and items restricts the performance of recommendation systems. To solve this problem, multi-modal technologies have been used for expanding available information. However, the existing multi-modal recommendation algorithms all extract the feature of single modality and simply splice the features of different modalities to predict the recommendation results. This fusion method can not completely mine the relevance of multi-modal features and lose the relationship between different modalities, which affects the prediction results. In this paper, we propose a Cross-Modal-Based Fusion Recommendation Algorithm (CMBF) that can capture both the single-modal features and the cross-modal features. Our algorithm uses a novel cross-modal fusion method to fuse the multi-modal features completely and learn the cross information between different modalities. We evaluate our algorithm on two datasets, MovieLens and Amazon. Experiments show that our method has achieved the best performance compared to other recommendation algorithms. We also design ablation study to prove that our cross-modal fusion method improves the prediction results.},
DOI = {10.3390/s21165275}
}



@Article{Fusion,
AUTHOR = {Li, Peishan and Zhan, Weixiao and Gao, Lutao and Wang, Shuran and Yang, Linnan},
TITLE = {Multimodal Recommendation System Based on Cross Self-Attention Fusion},
JOURNAL = {Systems},
VOLUME = {13},
YEAR = {2025},
NUMBER = {1},
ARTICLE-NUMBER = {57},
URL = {https://www.mdpi.com/2079-8954/13/1/57},
ISSN = {2079-8954},
ABSTRACT = {Recent advances in graph neural networks (GNNs) have enhanced multimodal recommendation systems’ ability to process complex user–item interactions. However, current approaches face two key limitations: they rely on static similarity metrics for product relationship graphs and they struggle to effectively fuse information across modalities. We propose MR-CSAF, a novel multimodal recommendation algorithm using cross-self-attention fusion. Building on FREEDOM, our approach introduces an adaptive modality selector that dynamically weights each modality’s contribution to product similarity, enabling more accurate product relationship graphs and optimized modality representations. We employ a cross-self-attention mechanism to facilitate both inter- and intra-modal information transfer, while using graph convolution to incorporate updated features into item and product modal representations. Experimental results on three public datasets demonstrate MR-CSAF outperforms eight baseline methods, validating its effectiveness in providing personalized recommendations, advancing the field of personalized recommendation in complex multimodal environments.},
DOI = {10.3390/systems13010057}
}




@article{ContrastiveLearning,
title = {A comprehensive survey on contrastive learning},
journal = {Neurocomputing},
volume = {610},
pages = {128645},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128645},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014164},
author = {Haigen Hu and Xiaoyuan Wang and Yan Zhang and Qi Chen and Qiu Guan},
keywords = {Contrastive learning, Representation learning, Self-supervised learning, Unsupervised learning},
abstract = {Contrastive Learning is self-supervised representation learning by training a model to differentiate between similar and dissimilar samples. It has been shown to be effective and has gained significant attention in various computer vision and natural language processing tasks. In this paper, we comprehensively and systematically sort out the main ideas, recent developments and application areas of contrastive learning. Specifically, we firstly provide an overview of the research activity of contrastive learning in recent years. Secondly, we describe the basic principles and summarize a universal framework of contrastive learning. Thirdly, we further introduce and discuss the latest advances of each functional component in detail, including data augmentation, positive/negative samples,network structure, and loss function. Finally, we summarize contrastive learning and discuss the challenges, future research trends and development directions in the area of contrastive learning.}
}


@inproceedings{CLIP,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}


@inproceedings{AudioCLIP,
  title={Audioclip: Extending clip to image, text and audio},
  author={Guzhov, Andrey and Raue, Federico and Hees, J{\"o}rn and Dengel, Andreas},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={976--980},
  year={2022},
  organization={IEEE}
}


@article{CLIPRecSys,
  title={Large multi-modal encoders for recommendation},
  author={Yi, Zixuan and Long, Zijun and Ounis, Iadh and Macdonald, Craig and Mccreadie, Richard},
  journal={arXiv preprint arXiv:2310.20343},
  year={2023}
}