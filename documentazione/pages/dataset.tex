\section{Dataset}

The dataset used in this project is \textbf{MovieLens 1M}, a collection of explicit movie ratings widely used in recommender systems research.

\subsection{Dataset Characteristics}

MovieLens 1M contains:
\begin{itemize}
    \item \textbf{1,000,209 explicit ratings} on a 1--5 star scale
    \item \textbf{6,040 users} with demographic data
    \item \textbf{3,952 movies} with associated metadata
    \item Collection period: approximately 2000--2001
\end{itemize}

\subsection{User Metadata}

For each user, the following demographic information is available:
\begin{itemize}
    \item \textbf{User ID}: unique identifier
    \item \textbf{Gender}: M (male) or F (female)
    \item \textbf{Age}: divided into ranges (e.g., 18-24, 25-34, 35-44, 45-49, 50-55, 56+)
    \item \textbf{Occupation}: encoded in predefined categories (e.g., student, programmer, manager, etc.)
    \item \textbf{ZIP Code}: residential zip code
\end{itemize}

\subsection{Movie Metadata}

For each movie, the following information is available:
\begin{itemize}
    \item \textbf{Movie ID}: unique identifier
    \item \textbf{Title}: includes release year in parentheses
    \item \textbf{Genres}: one or more associated genres (Action, Comedy, Drama, etc.)
\end{itemize}

\subsection{Multimodal Extension}

In the context of this project, the MovieLens 1M dataset has been enriched with multimodal data for each movie:

\begin{itemize}
    \item \textbf{Images}: movie posters or representative images (.jpg/.png format)
    \item \textbf{Audio}: audio clips or associated soundtracks (.wav format)
    \item \textbf{Texts}: textual descriptions, synopses, or reviews (.txt format)
\end{itemize}

All multimodal files are organized with the same basename corresponding to the Movie ID, ensuring correspondence between different modalities. For example, for the movie with ID \texttt{1}:
\begin{itemize}
    \item Image: \texttt{ml1m/\_images/1.jpg}
    \item Audio: \texttt{ml1m/\_audios/1.wav}
    \item Text: \texttt{ml1m/\_texts/1.txt}
\end{itemize}

\subsection{Multimodal Embeddings}

The multimodal embeddings were extracted using the \textbf{AudioCLIP} model, a multimodal extension of CLIP that supports images, audio, and text in a shared embedding space with consistent dimensionality.

\subsubsection{Extraction Process}

The embedding extraction process is implemented in the \texttt{extract\_audioclip\_embeddings.py} script, which ensures robust handling of multimodal data through the following pipeline:

\paragraph{File Discovery and Alignment}
The script first scans three directories (\texttt{\_images}, \texttt{\_audios}, \texttt{\_texts}) and identifies files by their basename (e.g., movie ID). From the initial scan:
\begin{itemize}
    \item 3,196 image files were found
    \item 3,535 audio files were found
    \item 3,197 text files were found
    \item 3,635 unique basenames across all modalities
\end{itemize}

The script performs an \textbf{intersection} of basenames to ensure only movies with \textit{all three modalities} are processed, resulting in \textbf{3,096 candidate items}.

\paragraph{Per-Modality Extraction with Error Handling}

\textbf{Image Embeddings:}
\begin{itemize}
    \item Preprocessing: resize to 256px, center crop to 224×224, normalization with CLIP statistics
    \item Batch processing via \texttt{model.encode\_image}
    \item L2 normalization of output vectors
    \item Error handling: corrupt images are logged and excluded
\end{itemize}

\textbf{Audio Embeddings (Sliding Window Mean):}
\begin{itemize}
    \item Sample rate: 44.1 kHz (resampled if necessary)
    \item Sliding window approach with configurable parameters:
    \begin{itemize}
        \item Window length: 2.0 seconds (88,200 samples)
        \item Stride: 1.0 seconds (50\% overlap)
        \item Short audio handling: padded to minimum window length
    \end{itemize}
    \item Each window is encoded independently via \texttt{model.encode\_audio}
    \item Final embedding: \textbf{mean aggregation} across all windows
    \item Error handling: corrupted or empty audio files are logged and excluded
\end{itemize}

\textbf{Text Embeddings (Adaptive Chunking):}
\begin{itemize}
    \item CLIP tokenizer has a hard limit of 77 tokens per input
    \item For texts exceeding this limit:
    \begin{enumerate}
        \item Split text into sentences (by `.`, `!`, `?`)
        \item Greedily group sentences into chunks that fit within 77 tokens
        \item If a chunk still exceeds the limit, iteratively reduce by 20\% until valid
        \item Encode each chunk via \texttt{model.encode\_text}
        \item Final embedding: \textbf{mean aggregation} across all chunks
    \end{enumerate}
    \item L2 normalization applied to all outputs
    \item Error handling: unreadable files or processing failures are logged and excluded
\end{itemize}

\paragraph{Consistency Enforcement}
After extraction, the script enforces \textbf{strict alignment}:
\begin{itemize}
    \item Only items successfully extracted in \textit{all three modalities} are retained
    \item Failed extractions (e.g., 1 audio file corrupted) result in removal from all modalities
    \item Final output arrays have \textbf{identical row counts} and order
\end{itemize}

In the current extraction:
\begin{itemize}
    \item 3,096 items had all three modalities available
    \item 1 audio file failed extraction (corrupted EOF)
    \item Final dataset: \textbf{3,095 items} with complete embeddings
\end{itemize}

\subsubsection{Output Files}

The extracted embeddings are saved in NumPy format (.npy) in the \texttt{features\_mmrec/} folder:

\begin{itemize}
    \item \texttt{images.npy}: visual embeddings, shape $(3095, 1024)$
    \item \texttt{audios.npy}: audio embeddings, shape $(3095, 1024)$
    \item \texttt{texts.npy}: textual embeddings, shape $(3095, 1024)$
    \item \texttt{item\_features.csv}: mapping between Movie ID and array index
\end{itemize}

All three \texttt{.npy} files have:
\begin{itemize}
    \item \textbf{Identical number of rows} (3,095)
    \item \textbf{Identical row order}: row $i$ in each file corresponds to the same movie
    \item \textbf{Same embedding dimension} (1,024), matching AudioCLIP's output space
    \item \textbf{L2-normalized vectors}: $\|\mathbf{v}\|_2 = 1$ for all embeddings
\end{itemize}

\paragraph{Item-to-Index Mapping (item\_features.csv)}
The CSV file provides a lookup table to retrieve embeddings by Movie ID:

\begin{verbatim}
item_id,idx
10,0
100,1
1000,2
...
\end{verbatim}

To access embeddings for movie ID \texttt{1000}:
\begin{enumerate}
    \item Lookup \texttt{idx=2} in \texttt{item\_features.csv}
    \item Load \texttt{images[2]}, \texttt{audios[2]}, \texttt{texts[2]}
    \item All three vectors are guaranteed to correspond to the same movie
\end{enumerate}

\subsection{Multimodal Dataset Statistics}

\begin{table}[h]
\centering
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Count} \\
\hline
Total unique movie IDs & 3,635 \\
Movies with all 3 modalities & 3,096 \\
Extraction failures & 1 (audio) \\
\textbf{Final dataset size} & \textbf{3,095} \\
Embedding dimension & 1,024 \\
\hline
\end{tabular}
\caption{Multimodal dataset statistics after extraction and alignment}
\end{table}

The final dataset represents \textbf{78.3\%} of the original MovieLens 1M catalog (3,095 out of 3,952 movies), limited by the availability of complete multimodal data (images, audio, text) and successful extraction across all modalities.




File trovati: 3196 immagini, 3535 audio, 3197 testi

Analisi basename: 3635 nomi unici totali
  ⚠ 439 nomi senza immagine. Esempi: ['1014', '1019', '1039', '1045', '1052', '1058', '106', '1073', '109', '1106']
  ⚠ 100 nomi senza audio. Esempi: ['108', '1142', '1164', '121', '1257', '1348', '137', '138', '1420', '1421']
  ⚠ 438 nomi senza testo. Esempi: ['1014', '1019', '1039', '1045', '1052', '1058', '106', '1073', '109', '1106']

✓ Trovati 3096 campioni con TUTTE E 3 le modalità. Esempio: ['10', '100', '1000', '1002', '1003']